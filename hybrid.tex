\documentclass[amsmath]{lni}

\usepackage{xcolor}
\usepackage[ruled]{algorithm2e}
% %\usepackage{algorithmic}
%\usepackage{amsmath}
% \usepackage{amsfonts}
\let\openbox\undefined
\usepackage{amsthm}
% \usepackage{graphicx}
\newtheorem{theorem}{Theorem}
% \newtheorem{cxampl}[theorem]{Counterexample}
\newtheorem{lemma}[theorem]{Lemma}

\newcommand{\TODO}[1]{\begingroup\color{red}#1\endgroup}
\newcommand{\NR}[1]{\begingroup\color{orange}#1\endgroup}
\newcommand{\PFS}[1]{\begingroup\color{green}#1\endgroup}

\begin{document}


\title[Phylogenetics with Mixing Hybrids]{Reconstruction of Language
  Phylogenies with ``Mixing''}

\author[Nancy Retzlaff \and Peter F.\ Stadler] {Nancy
  Retzlaff\footnote{Universität, Abteilung, Straße, Postleitzahl Ort, Land
    \email{emailaddress@author1}}
  \and
%
  Peter F.\ Stadler\footnote{Dept.\ of Computer Science and
    Interdisciplinary Center for Bioinformatics, Leipzig University,
    H{\"a}rtelstra{\ss}e 16-18, 04109 Leipzig, Germany; Max Planck
    Institute for Mathematics in the Sciences, Inselstra{\ss}e 22, D-04103
    Leipzig, Germany; Dept.\ of Theoretical Chemistry, University of
    Vienna, W{\"a}hringerstra{\ss}e 17, A-1090 Wien, Austria; Facultad de
    Ciencias, Universidad National de Colombia, Bogot{\'a}, Colombia; Santa
    Fe Institute, 1399 Hyde Park Road, Santa Fe NM 87501, USA
    \email{studla@tbi.univie.ac.at}}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\startpage{1} % Beginn der Seitenzählung für diesen Beitrag / Start page
\editor{Herausgeber et al.} % Names of Editors
\booktitle{Methoden und Anwendungen der Computational Humanities} % Name of book title
\year{2020}
%%% \lnidoi{18.18420/provided-by-editor-02} % if known
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\maketitle


\begin{abstract}
  super-cool abstract
\end{abstract}

\begin{keywords}
Schlagwort1 \and Schlagwort2 %Keyword1 \and Keyword2
\end{keywords}

\section{Introduction}

Phylogenetic approaches, that is, the reconstruction of historical
relationships from present-day observations has a long history in both
linguistics and biology. Most commonly, a tree models is assumed in which
lineages separate and then evolve essentially independently of each other.
While this is a very good approximation in most situations in both
biological and language evolution (borrowing nonwithstanding) there are
clear exceptions that cannot be captured by the tree model. In biology,
hybridization -- the production of offspring by mating two parents from
different varieties or species -- is not in particular in plants. In
languages, creoles or the English vocabulary serve as example. Albeit a
Germanic language, the English lexicon contains 29\% Latin and 29\% French
words, while only 25\% trace back to its Germanic origin
\cite{Finkenstaedt:73}.

As time proceeds, evolutionary events such as mutations of DNA nucleotides
and lineage-specific loss of genes or cognate words slowly increses the
dissimilarity of the evolving entities. In the tree model, so-called
additive distance measures $d(x,y)$ keep track of the number of events that
separate species or languages $x$ and $y$ from each other.  It is well
known that a unique tree can reconstructed from additive distance data
\cite{SimoesPereira:69}. Real-life data, however, are rarely if ever
additive due to unavoidable noise and measurement errors, although they are
often ``tree-like'' enough to allow the inference of a tree that is a least
a very good approximation to the true evolutionary history
\cite{Atteson:99}.

In the presence of the hybridization/mixing events, however, methods that
reconstruct trees must fail. In \cite{Prohaska:17a}, we introduced a simple
extension of the tree model describing the evolution of gene clusters by a
process known as non-homologous crossover, which leads to new genes whose
sequences consist of parts inherited from two ancestors that are adjacent
in the genome. The same model for the evolution of distances also applies
to mixing events in general by just relaxing the constraint that the
ancestors are adjacent on a linear genome, see Section~\ref{sect:R}. In
\cite{Prohaska:17a} we also devised a recognition algorithm that implicitly
reconstructs a sequence of divergence and mixing events that explains a
given distance matrix. Here we generalize this idea and provide an
approximation algorithm that is capable of inferring a sequence of events
also from imperfect data.

\section{Modelling Distances in the Presence of Mixing} 
\label{sect:R}

We consider evolution as an iterative process. Instead of speaking of genes
or languages specifically, we follow the convention of the phylogenetics
literature use the neutral term \emph{taxon}. Each taxon is associated with
data, such as the genomic DNA sequence of representative organism, a
collection of grammatics features, or a word list. With time these data
slowly change independently of each other, thus increasing the distance
$d(x,y)$ between two taxa by increments $\delta_x$ and $\delta_y$ that are
determined by each taxons individual rate of change. Once in a while, a
taxon is subdivided into two separate lineages that henceforth will evolve
independently of each other. This amounts to introducing a new taxon $z$ as
an ``offspring'' of $x$, which initially is virtually indistinguishable
from $x$. This is $d(x,z)=(x,y)$ for all taxa $y\ne x,z$ and $d(z,x)=0$.
After this ``speciation'' event, each taxon evolves independently, thus
incrementing the distances. It is not difficult to prove that this process
generates an additive distance matrix, and all additive distances matrices
can be generated in the manner \cite{Prohaska:17a}. Therefore, this simple
process amounts to the tree model, expressed in terms of the observable
distances between taxa.

A mixing or hybridization event can also be modeled in this setting
\cite{Prohaska:17a}. A hybrid $z$ is a mixture of two parents $x$ and $y$,
with $x$ contributing a fraction $\alpha$ and $y$ contributing the
remaining fraction $(1-\alpha)$. Consider the distances immediately after
the mixing event, i.e., before the lineages have time to acquire additional
changes. Then $d(x,z)=\alpha 0 + (1-\alpha) d(x,y)$ and
$d(y,z) = \alpha d(x,y) + (1-\alpha 0)$, since only the part of $z$
deriving from $y$ contributes to the distance $d(x,z)$ and \textit{vice
  versa}. The distance of any other taxon $u$ to the hybrid $z$ is is the
weighted average $d(u,z)=\alpha d(u,x)+(1-\alpha) d(u,y)$ of $u$ to its
parents $x$ and $y$. Note that for $\alpha=0$, $z$ is an identical
offspring of $y$, while for $\alpha=1$ it is an identical offspring of
$x$. The tree model outline in the previous paragraph thus is a special
case.

More formally we describe each step as follows: pick a pair of parents
$x,y\in V$ and a mixing ration $\alpha$, and pick distance
increments $\delta_u$ for all $u\in V\cup\{z\}$. Then add $z$ to $V$
and compute updated distances $d'$ as follows: 
\begin{equation} 
\begin{split} 
  d_{zx}' & = (1-\alpha)  d_{xy} +\delta_x + \delta_z \\
  d_{zy}' & =   \alpha    d_{xy} +\delta_y + \delta_z  \\
  d_{zu}' & =   \alpha    d_{xu} + (1-\alpha)  d_{yu} + \delta_u + \delta_z
  \quad\textrm{for all}  u\in V\setminus\{ x,y,z \}\\
  d_{uv}' & = d_{uv}+\delta_u+\delta_v
  \quad\textrm{for all}  u,v\in V\setminus\{ x,y,z \}
\end{split}
\end{equation}
it is not difficult to show that $d'$ is a metric distance measure whenever
$d$ was a metric distance. We say that a distance is an \emph{R-metric} if
it can be constructed step-by-by as outlined above. In particular, as we
have seen, every additive metric is an R-metric, i.e., R-metric are a
proper generalization of tree distances. 

Our goal is now to find a way to approximate a given distance as an
R-metric, and more precisely to give a sequence of steps of form
$(z\leftarrow x)$ and $(z\leftarrow x,y)$ in which offsprings where
produced by splitting from a single parent or by the mixing of two
parents. First we note that this cannot be done in an unambiguous manner
for less than five taxa if mixing steps may be involved. Numerical data
strongly suggest, however, that the reconstruction of the relative sequence
of events becomes unambigous beyond this unresolvable ancestral ``core''
\cite{Prohaska:17a}. 

Algorithm~\ref{alg:recogR} \cite{Prohaska:17a} in the appendix in essence
reverts the recursive construction outlined above. In each step, it tries
to identify a pair $(x,z)$ corresponding to a tree-like split of lineages,
or a triple $(x,y,z)$ corresponding to a merge event. In the latter case,
the mixing ratio $\alpha$ needs to be computed. In the general case, the
same result must be obtained independent of which ``outgroups'' beyond $z$
and the parents $x$ and $y$ are used, thus imposing constraints that are
used to identify the correct mixing events. There may by more than one
candidate event. However, these candidates must not involve common taxa. In
other words, for each lineage, the order of speciation and mixing events
that it participates in can be determined from the distance data. We
emphasize that at present this is a conjecture supported by simulation. A
rigorous mathematical analysis of this issue is ongoing research.  Since
Algorithm~\ref{alg:recogR} works only for exact, error-free data. Here, we
stepwisely turn it into an approximation algorithms that, given a distance
matrix an input, computes a sequence of events and a corresponding
R-metric.

\section{From Recognition to Approximation} 





\TODO{Nancy, please check what of the following you have actually
  implemented.} 


Our goal here is to modify Alg.~\ref{alg:recogR} into a consistent
approximation algorithm. More precisely, we set out to construct an
algorithm $\mathbb{A}$ with the following properties:
\begin{itemize} 
  \item For every finite metric $d$ on $|V|$ points, $\mathbb{A}(d)$ 
    is an R-metric
  \item If $d$ is a R-metric, then $\mathbb{A}(d)=d$.
\end{itemize} 
In practise the subdivide $\mathbb{A}$ into two steps: In the first stage,
we extract a scenario $\mathcal{S}$ of merge and branch events from $d$. In
the second stage, this scenario is converted back into a metric distance
$d(\mathcal{S})$.

The non-trivial part is the inference of a scenario $\mathcal{S}$ from an
arbitrary metric $d$ on $V$.


Given a metric distance matrix $d$, we say
that a candidate event $(x,y:z)$ is \emph{perfect} if $a_{uv}$ as defined
in Alg.~\ref{alg:recogR} is the same for all $u,v\in V\setminus\{x,y,z\}$
and satisfied $a\in [0,1]$. If $a=1$ or $a=0$, we speak of perfect branch
candidate $(x:z)$ or $(y:z)$, if $0<a<1$, we speak of a perfect merge
candidate. The basic idea is now to iteratively identify candidates that
are as close to perfect as possible and to reduce the matrix by the best
branch or merge candidate. This overall logic is summarized in
Alg.\ref{alg:toplevel}.

\begin{algorithm}[H]
\caption{Consistent Approxmation of R-metrics}
\label{alg:toplevel}
\SetAlgoLined
\While{$|V|\ge 4$}{
  determine best branch step $(x:z)$\;
  determine best merge step $(x,y:z)$\;
  perform the best of split $(x:z)$ or merge $(x,y:z)$ step\;
  remove $z$ from and $V$\; 
}
compute initial branch scenario for $|V|\le 3$ leaves\;
\Return $\mathcal{S}$ 
\end{algorithm} 

Clearly, if $d$ is a R-metric, then there is always a perfect terminal
event that is recognized by Alg.\ref{alg:toplevel}, allowing a reduction of
the matrix by undoing the terminal merge or branch event and removing the
vertex $z$. Naturally, we obtain an approximation scheme that eventually
returns scenario $\mathcal{S}$ be settling for the best candidate in each
iteration step. As we shall see, some precautions have to be taken to
ensure that the scenario stays valid in the non-perfect case. More
importantly, it is not obvious what exactly the ``best candidate'' is
supposed to be. Before we address this issue in detail, we first describe
the reduction steps given that a candidate event has been selected in a
given iteration.

First we consider the special case of a branch steps. By Equ.(9) of
\cite{Prohaska:17a}, we have $a=0$ iff $d'_{uz}-d'_{vz} = d'_{uy}-d'_{vy}$
for all $y$ and $a=1$ iff $d'_{uz}-d'_{vz} = d'_{ux}-d'_{vx}$. These two
cases are effectively the same, identifying $x$ and $y$, respectively, as
the ancestral branch from which $z$ is split off. For the branch event
$(x:z)$ we obtain, by setting $a=1$, that $d'_{xz}=\delta_x+\delta_z$ and
$d'_{ux}-d'_{uz}=\delta_x-\delta_z$, whence
$d'_{xz}+d'_{ux}-d'_{uz}=2\delta_x$ and
$d'_{xz}+d'_{uz}-d'_{ux}=2\delta_y$. For numerical stability it seems
advisable to first compute the average
\begin{equation} 
  C := \frac{1}{|V|-2} \sum_{u\in V\setminus\{x,y\}} (d'_{ux}-d'_{uz})
\end{equation} 
in order to obtain the consensus values $\delta_x = (d'_{xz}+C)/2$ and
$\delta_z = (d'_{xz}-C)/2$.  We note, finally, there is no difference
whether $z$ as branching off from $x$ or \textit{vice versa}, hence it
suffices to consider only the $x$ small than $z$ in input order. 

\begin{algorithm}[H]
\caption{Branch($x:z$)} 
\label{alg:branchstep}
\SetAlgoLined
estimate consensus value for $C:=(d'_{ux}-d'_{uz})$ for $u\in
V\setminus\{x,z\}$ \;
$\delta_x = (d'_{xz}+C)/2$\;
$\delta_z = (d'_{xz}-C)/2$\;
$\forall u\in V\setminus\{x,z\}$: $d_{ux} =  d_{ux}-\delta_x$\;
\end{algorithm} 

For a merge step $(x,y:z)$ we essentially follow the reduction outline in
Alg.~\ref{alg:recogR}. As in the case of $\delta_x$ and $\delta_z$ in
Alg.~\ref{alg:branchstep} there a terms that can be obtained using
arbitrary choices of $u\in V\setminus\{x,y,z\}$. In particular that
pertains to the mixing parameter $a$, the value of $Q$, and the derived
quantity $d_{xy}$. For non-perfect candidates, it is desirable to estimate
robust average or consensus values instead of picking a particular value.
In Alg.~\ref{alg:mergestep} we us a simplistic scheme in which we assume
that a consensus value $\hat a$ for the mixture parameter to be supplied
already with the best candidate $(x,y:z)$. Given $\hat a$, it is most
natural to estimate $Q$ as an arithmetic mean. All other parameters are
then uniquely determined.

\begin{algorithm}[H]
\caption{Merge($x,y:z$)} 
\label{alg:mergestep} 
\SetAlgoLined
use $a$ estimated for $(x,y:z)$\;
$\delta_z = \frac{1}{2}(d'_{xz}+d'_{yz}-d'_{xy})$\;
estimate consensus value for $Q:=(d'_{uz}-a d'_{ux}-(1-a) d'_{uy})$ for the 
set $u\in V\setminus\{x,y,z\}$\;
$d_{xy} = (Q-2 \delta_z + a d'_{xz}+(1-a) d'_{yz})/(2a(1-a))$\;
$\delta_x = d'_{xz} - (1-a) d_{xy} - \delta_z$\;
$\delta_y = d'_{yz} -   a   d_{xy} - \delta_z$\;
$\forall u\in V\setminus\{x,y,z\}$ and $\forall p\in\{x,y\}$ set   
$d_{up} = d_{pu} -\delta_p$\;
repair metric if necessary\;
\end{algorithm} 

The incremental modifications in Alg.~\ref{alg:branchstep} and
\ref{alg:mergestep} are not guaranteed to preserve metrics. Furthermore,
several other inequalities must hold and may need be enforced. First,
$\delta_x,\delta_y,\delta_z\ge 0$, $d_{xy}\ge 0$, and $d_{ux},d_{uy}\ge
0$. In order to enforce the triangle inequality after the editing step, one
could use one of several \emph{triangle fixing} algorithms that have been
designed to find the metric closest to a given dissimilarity measure, see
e.g.\ \cite{Sra:05,Brickel:08}. Recently, algorithms have appeared that
restrict themselves to fixing only triangles that violate the triangle
inequality \cite{Gilbert:17}. For our purposes, Algorithm 2 of
\cite{Gilbert:17} seems particularly appropriate.

\subsection{Ranking of Branch and Merge Scenarios}

Let us now turn to the problem of finding best candidates. First suppose
there are perfect branch or merge candidates. In this case it seems
advisable to follow the logic of clustering algorithms and to consider the 
ones with smallest distances first. This choice does not influence the
correctness of the recognition algorithm but should contribute to numerical
stability.

If there is no perfect candidate, however, our choice will affect the final
result. Ideally, we would like to select a sequence of candidates that
minimizes the overall difference between the input and the output metric.
This seems to be difficult at the very least since in practise we have to
make a choice in each iteration step. Therefore we resort to heuristics
that at least guarantee that perfect candidates are recognized as such.

For the branch candidates, Algorithm~\ref{alg:branchstep} is a rather
straightforward choice. It obviously ensures that a perfect branch step,
i.e., one with $\epsilon_{xz}=0$, receives highest priority.

\begin{algorithm}[H]
\caption{Find best branch candidate $(x:z)$ } 
\label{alg:branchstep}
\SetAlgoLined
\For{$\{x,z\} \subseteq V$}{
  \For{$\{u,v\}\in V\setminus\{x,z\}$}{ 
    $\epsilon_{xz} += \Vert (d_{vz}-d_{uz})- (d_{vx}-d_{ux}) \Vert$\;
  }   
}
\Return ranked list $[(x:z,\epsilon_{xz})| x,z\in V]$ by increasing 
$\epsilon_{xz}$\;
\end{algorithm} 

For merge candidates, the problem is bit more involved. The quality $q_a$
of the merge candidate $(x,y:z)$ is determined by how tightly
$a=(h_z-h_y)/(h_x-h_y)$ is centered at a single value $\hat a\in (0,1)$ for
all $u,v\in V\setminus\{x,y,z\}$. Since there is nothing to prevent the
denominator $h_x-h_y$ from becoming $0$, estimating $\hat a$ is also a not
entirely trivial problem.

A short computation shows that $a\in(0,1)$ if and only if $h_z$ lies
between $h_x$ and $h_y$. Correspondingly, we have $a<0$ for $h_z>h_y>h_x$
or $h_z<h_y<h_x$ and $a>1$ for $h_z>h_x>h_y$ or $h_z<h_x<h_y$.  This
suggests a step-wise evaluation:
\begin{itemize} 
\item[(i)] determine for the $\{u,v\}\in V\setminus\{x,y,z\}$ to which of
  three categories the triple $(h_x,h_y,h_z)$ belongs. If it is one of the
  latter two cases, reject the merge step and $(x,y:z)$ treat as one of of
  the branch steps $(x:z)$ or $(y:z)$.
\item[(ii)] If $a\in(0,1)$ we can estimate the mean ratio $a$ as the ration
  of the means of $h_z-h_y$ and $h_x-h_y$. This can in fact be done
  separately for the triples with $h_x>h_y$ and $h_y>h_x$ and use a
  weighted sum as the final result. Again, the merge step is rejected if
  the sum falls outside the interval $(0,1)$. 
\item[(iii)] For those merge candidates with $a\in(0,1)$, the standard
  deviation of $\alpha$ is used as the quality value $q$.
\end{itemize}
In the case of a perfect merge step, we have always have $h_z$ between
$h_x$ and $h_y$ and we obtain the same estimate for $a$, hence the standard
deviation is $0$, guaranteeing that the perfect merge candidates appear as
top candidates in the list of potential merges.

\begin{algorithm}[H]
\caption{Find best merging candidate $(x,y:z)$ } 
\label{alg:mergestep}

\For{$\{x,y,z\} \in V$}{
  $\forall \{u,v\} \in V\setminus\{x,y,z\}$ compute triples\\
  $h_x = d'_{ux}-d'_{vx}$, 
  $h_y = d'_{ux}-d'_{vx}$,
  $h_z = d'_{ux}-d'_{vx}$ \;
  compute best estimate for $\hat a$ and a quality measure $q_a$ 
  from $[h_x,h_y,h_z]$ \;
  \If{$\hat a<0$}{ $\hat a=0$, adjust $q_a$\;} 
  \If{$\hat a>1$}{ $\hat a=1$, adjust $q_a$\;}
}
\Return ranked list $[(x,y:z,a,q_a)| x,y,z\in V]$ by quality\;
\end{algorithm} 

It remains to determine a rational scheme for comparting merge and branch
steps.

\section{Showcase Application}

\TODO{Nancy}

\section{Stuff the needs to be done} 

\bibliography{hybrid}   
\newpage

\begin{algorithm}[H]
\caption{Recognition of R-metrics}
\label{alg:recogR}
\SetAlgoLined

\While{$|V|>3$}{
  \For{$\{x,y,z\} \in V$}{
    \For{$\{u,v\} \in V\setminus\{x,y,z\}$}{
      $a_{u,v} := 
      \frac{(d'_{uz}+d'_{vy})-(d'_{vz}+d'_{uy})}{(d'_{ux}+d'_{vy})-(d'_{vx}+d'_{uy})}$\;
    }
    \If{$\forall a_{u,v}: a_{u,v}=\alpha$ \,\&\, $\alpha\in [0,1]$}{
      \If{$\alpha \neq 0,1$}{
        \tcc{merge $(x,y:z)$} 
        $\delta_z = \frac{1}{2}(d'_{xz}+d'_{yz}-d'_{xy})$\;
        $d_{xy} = \frac{(d'_{uz}-a d'_{ux}-(1-a) d'_{uy})-
          2 \delta_z + a d'_{xz}+(1-a) d'_{yz}}{2a(1-a)}$\;
        $\delta_x = d'_{xz} - (1-a) d_{xy} - \delta_z$\;
        $\delta_y = d'_{yz} -   a   d_{xy} - \delta_z$\;
        $\forall u\in V\setminus\{x,y,z\}$ and $\forall p\in\{x,y\}$ set   
        $d_{up} = d_{pu} -\delta_p$\;
      }
      \tcc{If $a=1$ then split $(x:z)$} 
      \tcc{If $a=0$ then split $(y:z)$}
%      \TODO{why are the things above in a comment?} Because for
%      recognition only we don't do anything except dropping $z$
      $V\leftarrow V\setminus\{z\}$\;
      \textbf{continue}\;
    }  
  }
  \tcc{ no split or merge found for $\{x,y,z\}$ } 
  \Return \emph{\textbf{false}}
}
\Return \emph{\textbf{true}}
\end{algorithm}



\end{document}








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

LEYCHENTHEYLE 

\begin{algorithm}[H]
\caption{Approximation of an R-metric} 
\SetAlgoLined
\For{$\{x,y,z\} \in V$}{
  \For{$\{u,v\} \in V\setminus\{x,y,z\}$}{
   $a_{u,v} := \frac{(d'_{uz}+d'_{vy})-(d'_{vz}+d'_{uy})}{(d'_{ux}+d'_{vy})-(d'_{vx}+d'_{uy})}$\;
  }
  $\bar{a}_{xyz} = \frac{2}{(n-3)(n-4)} * a_{uv}$\;
  $\sigma_{xyz} = sdv(a_{uv})$\;  
 }
 sort by $\sigma_{xyz}$ in increasing order\;
 \tcc{select greedy}
 \If{$\bar{a}_{xyz} \in [0,1]$}{accept\;}
 \If{$-\sigma_{xyz} < \bar{a}_{xyz} < 0 $}{accept\;}
 \If{$1 < \bar{a}_{xyz} < 1+\sigma_{xyz}$}{accept\;}
 
 \tcc{now we know the best $\{x,y,z\}$ and need to remove $z$}
 
 \For{$\{u,v\} \in V\setminus\{x,y,z\}$}{
 
   \If{$\forall a_{u,v}: a_{u,v}=a\in [0,1]$}{
   \eIf{$a \neq 0,1$}{
   $\delta_z = \frac{1}{2}(d'_{xz}+d'_{yz}-d'_{xy})$\;
   $d_{xy} = \frac{(d'_{uz}-a d'_{ux}-(1-a) d'_{uy})-2 \delta_z + a d'_{xz}+(1-a) d'_{yz}}{2a(1-a)}$\;
   $\delta_x = d'_{xz} - (1-a)*d_{xy} - \delta_z$\;
   $\delta_y = d'_{yz} - a*\delta_z$\;
  }{
   \If{$\bar{a}_{xyz} < 0$} {accept $\bar{a}_{xyz} = 0$\;}
   \If{$\bar{a}_{xyz} > 1$} {accept $\bar{a}_{xyz} = 1$\;}
  }
  \NR{$\forall p,q\in V, p =\{x,y\}: d_{pq} = d_{pq}-\delta_p$}\;
  }
  }
\end{algorithm}
